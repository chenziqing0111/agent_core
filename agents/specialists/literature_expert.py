# agent_core/agents/specialists/literature_expert.py - åŸºäºRAGçš„æ–‡çŒ®åˆ†æä¸“å®¶

"""
ğŸ§¬ Literature Expert - æ–‡çŒ®åˆ†æä¸“å®¶
æ”¯æŒRAGä¼˜åŒ–çš„å¤§è§„æ¨¡æ–‡çŒ®åˆ†æï¼Œå¤§å¹…èŠ‚çœTokenæ¶ˆè€—
"""

import sys
import os
import asyncio
import hashlib
import pickle
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# æ ¸å¿ƒä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import faiss
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å®‰è£…: pip install sentence-transformers faiss-cpu")
    raise e

from Bio import Entrez
import xml.etree.ElementTree as ET

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from agent_core.clients.llm_client import call_llm
from agent_core.config.analysis_config import AnalysisConfig, AnalysisMode, ConfigManager

logger = logging.getLogger(__name__)

# ===== æ•°æ®ç»“æ„å®šä¹‰ =====

@dataclass
class LiteratureDocument:
    """æ–‡çŒ®æ–‡æ¡£ç»“æ„"""
    pmid: str
    title: str
    abstract: str
    authors: List[str] = None
    journal: str = ""
    year: int = 0
    doi: str = ""
    
    def __post_init__(self):
        if self.authors is None:
            self.authors = []
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºå¯æœç´¢çš„æ–‡æœ¬"""
        return f"æ ‡é¢˜: {self.title}\næ‘˜è¦: {self.abstract}"

@dataclass
class TextChunk:
    """æ–‡æœ¬å—ç»“æ„"""
    text: str
    doc_id: str  # PMID
    chunk_id: str
    metadata: Dict
    
    def __post_init__(self):
        if not self.chunk_id:
            self.chunk_id = hashlib.md5(f"{self.doc_id}_{self.text[:50]}".encode()).hexdigest()[:12]

@dataclass
class LiteratureAnalysisResult:
    """æ–‡çŒ®åˆ†æç»“æœ"""
    gene_target: str
    disease_mechanism: str
    treatment_strategy: str
    target_analysis: str
    references: List[Dict]
    total_literature: int
    total_chunks: int
    confidence_score: float
    analysis_method: str
    timestamp: str
    config_used: Dict
    token_usage: Dict

# ===== PubMedæ£€ç´¢å™¨ =====

class PubMedRetriever:
    """PubMedæ–‡çŒ®æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.name = "PubMed Retriever"
        self.version = "2.0.0"
        # é…ç½®Bio.Entrez
        Entrez.email = "czqrainy@gmail.com"
        Entrez.api_key = "983222f9d5a2a81facd7d158791d933e6408"
    
    async def search_literature(self, gene: str, max_results: int = 500) -> List[LiteratureDocument]:
        """æ£€ç´¢æ–‡çŒ®"""
        
        print(f"ğŸ“š æ£€ç´¢ {gene} ç›¸å…³æ–‡çŒ®ï¼Œç›®æ ‡: {max_results} ç¯‡")
        
        # å¤šç­–ç•¥æœç´¢
        search_strategies = [
            f"{gene}[Title/Abstract]",
            f'"{gene}" AND (disease OR treatment OR therapy)',
            f"{gene} AND (clinical trial[Publication Type] OR clinical study[Publication Type])",
            f"{gene} AND (mechanism OR pathway OR function)",
            f"{gene} AND (drug OR inhibitor OR target OR therapeutic)"
        ]
        
        all_documents = []
        seen_pmids = set()
        
        for strategy in search_strategies:
            print(f"  ğŸ” æœç´¢ç­–ç•¥: {strategy}")
            
            try:
                docs = await self._execute_search(strategy, max_results // len(search_strategies))
                
                for doc in docs:
                    if doc.pmid not in seen_pmids:
                        seen_pmids.add(doc.pmid)
                        all_documents.append(doc)
                        
                        if len(all_documents) >= max_results:
                            break
                
                print(f"    âœ… æ–°å¢ {len(docs)} ç¯‡ï¼Œç´¯è®¡ {len(all_documents)} ç¯‡")
                
                if len(all_documents) >= max_results:
                    break
                    
            except Exception as e:
                print(f"    âŒ æœç´¢å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“Š æ£€ç´¢å®Œæˆ: å…± {len(all_documents)} ç¯‡æ–‡çŒ®")
        return all_documents[:max_results]
    
    async def _execute_search(self, query: str, max_results: int) -> List[LiteratureDocument]:
        """æ‰§è¡Œå•æ¬¡æœç´¢"""
        
        try:
            # 1. æœç´¢PMID
            search_handle = Entrez.esearch(
                db="pubmed", 
                term=query, 
                retmax=max_results,
                sort="relevance"
            )
            search_results = Entrez.read(search_handle)
            pmid_list = search_results["IdList"]
            
            if not pmid_list:
                return []
            
            # 2. æ‰¹é‡è·å–è¯¦æƒ…
            documents = []
            batch_size = 50
            
            for i in range(0, len(pmid_list), batch_size):
                batch_pmids = pmid_list[i:i+batch_size]
                batch_docs = await self._fetch_batch_details(batch_pmids)
                documents.extend(batch_docs)
                
                # APIé™æµ
                if i + batch_size < len(pmid_list):
                    await asyncio.sleep(0.5)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æœç´¢æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    async def _fetch_batch_details(self, pmid_list: List[str]) -> List[LiteratureDocument]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…"""
        
        try:
            fetch_handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmid_list),
                rettype="medline",
                retmode="xml"
            )
            
            root = ET.fromstring(fetch_handle.read())
            
            documents = []
            for article in root.findall(".//PubmedArticle"):
                doc = self._parse_article(article)
                if doc and doc.abstract:  # åªä¿ç•™æœ‰æ‘˜è¦çš„
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_article(self, article_xml) -> Optional[LiteratureDocument]:
        """è§£æå•ç¯‡æ–‡ç« """
        
        try:
            # åŸºæœ¬ä¿¡æ¯
            pmid = article_xml.findtext(".//PMID", "")
            title = article_xml.findtext(".//ArticleTitle", "")
            
            # æ‘˜è¦å¤„ç†
            abstract_elem = article_xml.find(".//Abstract")
            abstract = ""
            if abstract_elem is not None:
                abstract_texts = []
                for text_elem in abstract_elem.findall(".//AbstractText"):
                    text = text_elem.text or ""
                    label = text_elem.get("Label", "")
                    if label:
                        abstract_texts.append(f"{label}: {text}")
                    else:
                        abstract_texts.append(text)
                abstract = " ".join(abstract_texts)
            
            # ä½œè€…
            authors = []
            for author in article_xml.findall(".//Author"):
                last_name = author.findtext("LastName", "")
                first_name = author.findtext("ForeName", "")
                if last_name:
                    authors.append(f"{first_name} {last_name}".strip())
            
            # æœŸåˆŠå’Œå¹´ä»½
            journal = article_xml.findtext(".//Journal/Title", "")
            year_elem = article_xml.find(".//PubDate/Year")
            year = int(year_elem.text) if year_elem is not None and year_elem.text else 0
            
            # DOI
            doi = ""
            for article_id in article_xml.findall(".//ArticleId"):
                if article_id.get("IdType") == "doi":
                    doi = article_id.text or ""
                    break
            
            if not title or not abstract:
                return None
            
            return LiteratureDocument(
                pmid=pmid,
                title=title,
                abstract=abstract,
                authors=authors,
                journal=journal,
                year=year,
                doi=doi
            )
            
        except Exception as e:
            return None

# ===== æ–‡æœ¬åˆ†å—å™¨ =====

class SmartChunker:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 250, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_documents(self, documents: List[LiteratureDocument]) -> List[TextChunk]:
        """åˆ†å—æ–‡æ¡£"""
        
        print(f"ğŸ“ å¼€å§‹æ–‡æœ¬åˆ†å—ï¼Œå—å¤§å°: {self.chunk_size}")
        
        all_chunks = []
        for doc in documents:
            chunks = self._chunk_single_doc(doc)
            all_chunks.extend(chunks)
        
        print(f"âœ… åˆ†å—å®Œæˆ: {len(documents)} ç¯‡ â†’ {len(all_chunks)} å—")
        return all_chunks
    
    def _chunk_single_doc(self, doc: LiteratureDocument) -> List[TextChunk]:
        """åˆ†å—å•ä¸ªæ–‡æ¡£"""
        
        chunks = []
        
        # 1. æ ‡é¢˜å—ï¼ˆé‡è¦ï¼‰
        title_chunk = TextChunk(
            text=f"æ ‡é¢˜: {doc.title}",
            doc_id=doc.pmid,
            chunk_id=f"{doc.pmid}_title",
            metadata={
                "pmid": doc.pmid,
                "title": doc.title,
                "journal": doc.journal,
                "year": doc.year,
                "chunk_type": "title"
            }
        )
        chunks.append(title_chunk)
        
        # 2. æ‘˜è¦åˆ†å—
        abstract_chunks = self._chunk_abstract(doc)
        chunks.extend(abstract_chunks)
        
        return chunks
    
    def _chunk_abstract(self, doc: LiteratureDocument) -> List[TextChunk]:
        """åˆ†å—æ‘˜è¦"""
        
        abstract = doc.abstract
        if len(abstract) <= self.chunk_size:
            # çŸ­æ‘˜è¦ï¼Œæ•´ä½“ä½œä¸ºä¸€å—
            return [TextChunk(
                text=f"æ‘˜è¦: {abstract}",
                doc_id=doc.pmid,
                chunk_id=f"{doc.pmid}_abstract",
                metadata={
                    "pmid": doc.pmid,
                    "title": doc.title,
                    "journal": doc.journal,
                    "year": doc.year,
                    "chunk_type": "abstract"
                }
            )]
        
        # é•¿æ‘˜è¦ï¼ŒæŒ‰å¥å­åˆ†å—
        sentences = self._split_sentences(abstract)
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(TextChunk(
                        text=f"æ‘˜è¦: {current_chunk}",
                        doc_id=doc.pmid,
                        chunk_id=f"{doc.pmid}_abstract_{chunk_index}",
                        metadata={
                            "pmid": doc.pmid,
                            "title": doc.title,
                            "journal": doc.journal,
                            "year": doc.year,
                            "chunk_type": "abstract_part",
                            "part_index": chunk_index
                        }
                    ))
                    chunk_index += 1
                
                current_chunk = sentence
        
        # æœ€åä¸€å—
        if current_chunk:
            chunks.append(TextChunk(
                text=f"æ‘˜è¦: {current_chunk}",
                doc_id=doc.pmid,
                chunk_id=f"{doc.pmid}_abstract_{chunk_index}",
                metadata={
                    "pmid": doc.pmid,
                    "title": doc.title,
                    "journal": doc.journal,
                    "year": doc.year,
                    "chunk_type": "abstract_part",
                    "part_index": chunk_index
                }
            ))
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        
        sentences = []
        current = ""
        
        for i, char in enumerate(text):
            current += char
            if char in '.!?' and i + 1 < len(text) and text[i + 1] in ' \n':
                sentences.append(current.strip())
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        return [s for s in sentences if len(s) > 10]

# ===== å‘é‡å­˜å‚¨ç³»ç»Ÿ =====

class VectorStore:
    """å‘é‡å­˜å‚¨å’Œæ£€ç´¢"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.encoder = SentenceTransformer(model_name)
        self.index = None
        self.chunks = []
    
    def build_index(self, chunks: List[TextChunk]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        
        print(f"ğŸ” æ„å»ºå‘é‡ç´¢å¼•ï¼Œæ¨¡å‹: {self.model_name}")
        
        self.chunks = chunks
        texts = [chunk.text for chunk in chunks]
        
        print(f"  ğŸ“Š ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬å—...")
        embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # æ ‡å‡†åŒ–ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} å—, ç»´åº¦: {dimension}")
    
    def search(self, query: str, top_k: int = 15) -> List[Dict]:
        """æœç´¢ç›¸å…³å—"""
        
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")
        
        # ç¼–ç æŸ¥è¯¢
        query_embedding = self.encoder.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # æœç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                results.append({
                    "chunk": chunk,
                    "score": float(score),
                    "text": chunk.text,
                    "metadata": chunk.metadata
                })
        
        return results
    
    def save(self, file_path: str):
        """ä¿å­˜ç´¢å¼•"""
        
        save_data = {
            "chunks": self.chunks,
            "model_name": self.model_name
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        faiss.write_index(self.index, file_path + ".faiss")
        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {file_path}")
    
    def load(self, file_path: str) -> bool:
        """åŠ è½½ç´¢å¼•"""
        
        try:
            with open(file_path, 'rb') as f:
                save_data = pickle.load(f)
            
            self.chunks = save_data["chunks"]
            self.model_name = save_data["model_name"]
            self.index = faiss.read_index(file_path + ".faiss")
            
            print(f"ğŸ“‚ ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} å—")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False

# ===== RAGæŸ¥è¯¢å¤„ç†å™¨ =====

class RAGProcessor:
    """RAGæŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.query_templates = {
            "disease_mechanism": "è¯¥åŸºå› ä¸å“ªäº›ç–¾ç—…ç›¸å…³ï¼Ÿç–¾ç—…çš„å‘ç—…æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆä¸´åºŠéœ€æ±‚ï¼Ÿ",
            "treatment_strategy": "æœ‰å“ªäº›æ²»ç–—æ–¹æ³•å’Œç­–ç•¥ï¼ŸåŒ…æ‹¬è¯ç‰©ã€ç–—æ³•ç­‰ï¼Ÿä¸´åºŠç ”ç©¶ç°çŠ¶å¦‚ä½•ï¼Ÿ",
            "target_analysis": "è¯¥åŸºå› çš„ä½œç”¨é€šè·¯æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›æ½œåœ¨æ²»ç–—é¶ç‚¹ï¼Ÿç ”ç©¶è¿›å±•å¦‚ä½•ï¼Ÿ"
        }
    
    async def process_query(self, gene: str, query_type: str, top_k: int = 15) -> str:
        """å¤„ç†RAGæŸ¥è¯¢"""
        
        print(f"ğŸ¤– RAGæŸ¥è¯¢: {gene} - {query_type}")
        
        # æ„å»ºæŸ¥è¯¢
        base_query = self.query_templates.get(query_type, "")
        full_query = f"{gene} {base_query}"
        
        # æ£€ç´¢ç›¸å…³å—
        relevant_chunks = self.vector_store.search(full_query, top_k)
        
        if not relevant_chunks:
            return f"æœªæ‰¾åˆ°ä¸ {gene} ç›¸å…³çš„ {query_type} ä¿¡æ¯ã€‚"
        
        print(f"  ğŸ“Š æ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³å—")
        
        # æ„å»ºprompt
        prompt = self._build_prompt(gene, query_type, relevant_chunks)
        
        # LLMç”Ÿæˆ
        response = call_llm(prompt)
        return response
    
    def _build_prompt(self, gene: str, query_type: str, relevant_chunks: List[Dict]) -> str:
        """æ„å»ºRAG prompt"""
        
        # æ•´ç†ä¸Šä¸‹æ–‡
        context_blocks = []
        for i, chunk_info in enumerate(relevant_chunks):
            chunk = chunk_info["chunk"]
            score = chunk_info["score"]
            pmid = chunk.metadata.get("pmid", "")
            
            context_blocks.append(
                f"[{i+1}] (PMID: {pmid}, ç›¸ä¼¼åº¦: {score:.3f})\n{chunk.text}"
            )
        
        context_text = "\n\n".join(context_blocks)
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹æ„å»ºprompt
        if query_type == "disease_mechanism":
            prompt = f"""ä½ æ˜¯èµ„æ·±åŒ»å­¦ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯å›ç­”å…³äºåŸºå›  {gene} çš„é—®é¢˜ã€‚

é—®é¢˜ï¼šè¯¥åŸºå› æ¶‰åŠå“ªäº›ç–¾ç—…ï¼Ÿè¿™äº›ç–¾ç—…çš„å‘ç—…æœºåˆ¶æ˜¯æ€æ ·çš„ï¼Ÿæœ‰å“ªäº›å°šæœªæ»¡è¶³çš„ä¸´åºŠéœ€æ±‚ï¼Ÿ

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œè¯·ä½¿ç”¨ [1]ã€[2] ç­‰æ ‡è®°ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### ç–¾ç—…æœºåˆ¶ä¸ä¸´åºŠéœ€æ±‚ï¼ˆGene: {gene}ï¼‰
- ç–¾ç—…å…³è”ï¼š
- å‘ç—…æœºåˆ¶ï¼š
- ä¸´åºŠéœ€æ±‚ï¼š

æ³¨æ„ï¼šåªåŸºäºæä¾›çš„æ–‡çŒ®ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ æœªæåŠçš„å†…å®¹ã€‚"""

        elif query_type == "treatment_strategy":
            prompt = f"""ä½ æ˜¯åŒ»å­¦ä¸´åºŠé¡¾é—®ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯åˆ†æåŸºå›  {gene} ç›¸å…³çš„æ²»ç–—ç­–ç•¥ã€‚

é—®é¢˜ï¼šå½“å‰ä¸è¯¥åŸºå› ç›¸å…³çš„æ²»ç–—æ–¹æ³•æœ‰å“ªäº›ï¼ŸåŒ…æ‹¬ä¼ ç»Ÿæ²»ç–—ã€é¶å‘è¯ç‰©ã€å…ç–«æ²»ç–—ç­‰ã€‚

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œè¯·ä½¿ç”¨ [1]ã€[2] ç­‰æ ‡è®°ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### æ²»ç–—ç­–ç•¥åˆ†æï¼ˆGene: {gene}ï¼‰
- å·²æœ‰æ²»ç–—ç­–ç•¥ï¼š
- ä¸´åºŠç ”ç©¶ç°çŠ¶ï¼š
- ä¸è¯¥åŸºå› ç›´æ¥ç›¸å…³çš„å¹²é¢„æ–¹æ³•ï¼š

æ³¨æ„ï¼šåªåŸºäºæä¾›çš„æ–‡çŒ®ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ æœªæåŠçš„å†…å®¹ã€‚"""

        elif query_type == "target_analysis":
            prompt = f"""ä½ æ˜¯è¯ç‰©ç ”å‘ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯åˆ†æåŸºå›  {gene} çš„é¶ç‚¹æ½œåŠ›ã€‚

é—®é¢˜ï¼šä¸è¯¥åŸºå› ç›¸å…³çš„å…³é”®é€šè·¯ã€è›‹ç™½å¤åˆç‰©æˆ–ä¿¡å·æœºåˆ¶ï¼Ÿæœ‰å“ªäº›æ½œåœ¨å¹²é¢„ä½ç‚¹å€¼å¾—å…³æ³¨ï¼Ÿç ”ç©¶å¤„äºä½•ç§é˜¶æ®µï¼Ÿ

è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç›¸å…³æ–‡çŒ®æ®µè½ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚åœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œè¯·ä½¿ç”¨ [1]ã€[2] ç­‰æ ‡è®°ã€‚

ç›¸å…³æ–‡çŒ®æ®µè½ï¼š
{context_text}

è¯·ä»¥å¦‚ä¸‹ç»“æ„è¾“å‡ºï¼š
### é¶ç‚¹åˆ†æä¸ç ”ç©¶è¿›å±•ï¼ˆGene: {gene}ï¼‰
- ä½œç”¨é€šè·¯ï¼š
- æ½œåœ¨é¶ç‚¹ï¼š
- ç ”ç©¶çŠ¶æ€ï¼š

æ³¨æ„ï¼šåªåŸºäºæä¾›çš„æ–‡çŒ®ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ æœªæåŠçš„å†…å®¹ã€‚"""

        return prompt

# ===== ç¼“å­˜ç®¡ç†å™¨ =====

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "literature_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_path(self, gene: str, max_results: int) -> str:
        """è·å–ç¼“å­˜è·¯å¾„"""
        cache_key = f"{gene}_{max_results}"
        return os.path.join(self.cache_dir, f"{cache_key}")
    
    def is_valid(self, cache_path: str, max_age_days: int = 7) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        return datetime.now() - file_time < timedelta(days=max_age_days)
    
    def save(self, gene: str, max_results: int, vector_store: VectorStore):
        """ä¿å­˜ç¼“å­˜"""
        cache_path = self.get_cache_path(gene, max_results)
        vector_store.save(cache_path)
    
    def load(self, gene: str, max_results: int) -> Optional[VectorStore]:
        """åŠ è½½ç¼“å­˜"""
        cache_path = self.get_cache_path(gene, max_results)
        
        if self.is_valid(cache_path):
            vector_store = VectorStore()
            if vector_store.load(cache_path):
                print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½: {gene}")
                return vector_store
        
        return None

# ===== ä¸»è¦çš„Literature Expert =====

class LiteratureExpert:
    """æ–‡çŒ®åˆ†æä¸“å®¶ - åŸºäºRAGä¼˜åŒ–"""
    
    def __init__(self, config: AnalysisConfig = None):
        self.name = "Literature Expert"
        self.version = "2.0.0"
        self.expertise = ["æ–‡çŒ®åˆ†æ", "æœºåˆ¶ç ”ç©¶", "æ²»ç–—ç­–ç•¥", "é¶ç‚¹åˆ†æ"]
        
        # é…ç½®
        self.config = config or ConfigManager.get_standard_config()
        
        # ç»„ä»¶
        self.retriever = PubMedRetriever()
        self.chunker = SmartChunker(chunk_size=250, overlap=50)
        self.cache_manager = CacheManager()
        
        logger.info(f"Literature Expert åˆå§‹åŒ–å®Œæˆ - {self.version}")
    
    def set_config(self, config: AnalysisConfig):
        """è®¾ç½®é…ç½®"""
        self.config = config
        logger.info(f"é…ç½®å·²æ›´æ–°: {config.mode.value}")
    
    def set_mode(self, mode: AnalysisMode):
        """è®¾ç½®æ¨¡å¼"""
        self.config = ConfigManager.get_config_by_mode(mode)
        logger.info(f"æ¨¡å¼åˆ‡æ¢: {mode.value}")
    
    async def analyze(self, gene_target: str, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """
        ä¸»è¦åˆ†ææ–¹æ³•
        
        Args:
            gene_target: ç›®æ ‡åŸºå› 
            context: ä¸Šä¸‹æ–‡é…ç½®
        
        Returns:
            æ–‡çŒ®åˆ†æç»“æœ
        """
        
        logger.info(f"å¼€å§‹æ–‡çŒ®åˆ†æ: {gene_target} - æ¨¡å¼: {self.config.mode.value}")
        
        try:
            # ç¡®å®šåˆ†æå‚æ•°
            max_literature = self._get_max_literature()
            top_k = self._get_top_k()
            
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
            vector_store = self.cache_manager.load(gene_target, max_literature)
            
            # 2. å¦‚æœç¼“å­˜æ— æ•ˆï¼Œé‡æ–°æ„å»º
            if vector_store is None:
                vector_store = await self._build_literature_index(gene_target, max_literature)
                # ä¿å­˜ç¼“å­˜
                self.cache_manager.save(gene_target, max_literature, vector_store)
            
            # 3. RAGæŸ¥è¯¢å¤„ç†
            rag_processor = RAGProcessor(vector_store)
            
            print("ğŸ¤– å¼€å§‹RAGæŸ¥è¯¢...")
            
            # å¹¶å‘å¤„ç†ä¸‰ä¸ªæŸ¥è¯¢
            tasks = [
                rag_processor.process_query(gene_target, "disease_mechanism", top_k),
                rag_processor.process_query(gene_target, "treatment_strategy", top_k),
                rag_processor.process_query(gene_target, "target_analysis", top_k)
            ]
            
            results = await asyncio.gather(*tasks)
            disease_result, treatment_result, target_result = results
            
            # 4. æ„å»ºåˆ†æç»“æœ
            references = self._extract_references(vector_store.chunks)
            confidence_score = self._calculate_confidence(vector_store.chunks)
            
            analysis_result = LiteratureAnalysisResult(
                gene_target=gene_target,
                disease_mechanism=disease_result,
                treatment_strategy=treatment_result,
                target_analysis=target_result,
                references=references[:50],  # é™åˆ¶å¼•ç”¨æ•°é‡
                total_literature=len(set(chunk.doc_id for chunk in vector_store.chunks)),
                total_chunks=len(vector_store.chunks),
                confidence_score=confidence_score,
                analysis_method="RAG-optimized",
                timestamp=datetime.now().isoformat(),
                config_used=self._get_config_summary(),
                token_usage=self._estimate_token_usage(top_k)
            )
            
            logger.info(f"æ–‡çŒ®åˆ†æå®Œæˆ: {gene_target} - æ–‡çŒ®æ•°: {analysis_result.total_literature}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®åˆ†æå¤±è´¥: {gene_target} - {str(e)}")
            return self._create_error_result(gene_target, str(e))
    
    async def _build_literature_index(self, gene: str, max_results: int) -> VectorStore:
        """æ„å»ºæ–‡çŒ®ç´¢å¼•"""
        
        print(f"ğŸ—ï¸ æ„å»ºæ–‡çŒ®ç´¢å¼•: {gene}")
        
        # 1. æ£€ç´¢æ–‡çŒ®
        documents = await self.retriever.search_literature(gene, max_results)
        
        if not documents:
            raise ValueError(f"æœªæ‰¾åˆ° {gene} ç›¸å…³æ–‡çŒ®")
        
        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.chunker.chunk_documents(documents)
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        vector_store = VectorStore()
        vector_store.build_index(chunks)
        
        return vector_store
    
    def _get_max_literature(self) -> int:
        """è·å–æœ€å¤§æ–‡çŒ®æ•°é‡"""
        if self.config.mode == AnalysisMode.QUICK:
            return 100
        elif self.config.mode == AnalysisMode.STANDARD:
            return 500
        elif self.config.mode == AnalysisMode.DEEP:
            return 1000
        else:
            return 500
    
    def _get_top_k(self) -> int:
        """è·å–top-kå‚æ•°"""
        if self.config.mode == AnalysisMode.QUICK:
            return 10
        elif self.config.mode == AnalysisMode.STANDARD:
            return 15
        elif self.config.mode == AnalysisMode.DEEP:
            return 25
        else:
            return 15
    
    def _extract_references(self, chunks: List[TextChunk]) -> List[Dict]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        
        references = {}
        for chunk in chunks:
            pmid = chunk.metadata.get("pmid", "")
            if pmid and pmid not in references:
                references[pmid] = {
                    "PMID": pmid,
                    "Title": chunk.metadata.get("title", ""),
                    "Journal": chunk.metadata.get("journal", ""),
                    "Year": chunk.metadata.get("year", 0)
                }
        
        return list(references.values())
    
    def _calculate_confidence(self, chunks: List[TextChunk]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        
        if not chunks:
            return 0.0
        
        # åŸºäºæ–‡çŒ®æ•°é‡å’Œè´¨é‡çš„ç®€å•è¯„åˆ†
        unique_docs = len(set(chunk.doc_id for chunk in chunks))
        
        if unique_docs >= 50:
            return 0.9
        elif unique_docs >= 20:
            return 0.8
        elif unique_docs >= 10:
            return 0.7
        elif unique_docs >= 5:
            return 0.6
        else:
            return 0.5
    
    def _estimate_token_usage(self, top_k: int) -> Dict:
        """ä¼°ç®—Tokenä½¿ç”¨"""
        
        # RAGæ–¹å¼çš„Tokenä¼°ç®—
        input_tokens = top_k * 200  # æ¯ä¸ªç›¸å…³å—çº¦200 tokens
        output_tokens = 1000 * 3   # ä¸‰ä¸ªé—®é¢˜å„1000 tokensè¾“å‡º
        total_tokens = input_tokens + output_tokens
        
        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "estimated_cost_usd": total_tokens * 0.000002
        }
    
    def _get_config_summary(self) -> Dict:
        """è·å–é…ç½®æ‘˜è¦"""
        
        return {
            "mode": self.config.mode.value,
            "max_literature": self._get_max_literature(),
            "top_k": self._get_top_k(),
            "analysis_method": "RAG-optimized"
        }
    
    def _create_error_result(self, gene_target: str, error_msg: str) -> LiteratureAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        
        return LiteratureAnalysisResult(
            gene_target=gene_target,
            disease_mechanism=f"åˆ†æ {gene_target} æ—¶å‘ç”Ÿé”™è¯¯: {error_msg}",
            treatment_strategy="",
            target_analysis="",
            references=[],
            total_literature=0,
            total_chunks=0,
            confidence_score=0.0,
            analysis_method="error",
            timestamp=datetime.now().isoformat(),
            config_used=self._get_config_summary(),
            token_usage={}
        )
    
    def export_results(self, result: LiteratureAnalysisResult, format: str = "dict") -> Any:
        """å¯¼å‡ºç»“æœ"""
        
        if format == "dict":
            return asdict(result)
        elif format == "json":
            import json
            return json.dumps(asdict(result), indent=2, ensure_ascii=False)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    def estimate_analysis_cost(self, gene_target: str) -> Dict[str, Any]:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        
        token_estimate = self._estimate_token_usage(self._get_top_k())
        
        return {
            "gene_target": gene_target,
            "estimated_tokens": token_estimate["total_tokens"],
            "estimated_cost_usd": token_estimate["estimated_cost_usd"],
            "estimated_time_seconds": 60,  # RAGåˆ†æçº¦1åˆ†é’Ÿ
            "config_mode": self.config.mode.value,
            "max_literature": self._get_max_literature()
        }
    
    def __str__(self) -> str:
        return f"LiteratureExpert(name='{self.name}', version='{self.version}', mode='{self.config.mode.value}')"

# ===== ä½¿ç”¨ç¤ºä¾‹ =====

async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    print("ğŸ§¬ Literature Expert ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. åˆ›å»ºLiterature Expert
    expert = LiteratureExpert()
    expert.set_mode(AnalysisMode.STANDARD)
    
    print(f"ğŸ“š {expert.name} v{expert.version} å·²å¯åŠ¨")
    print(f"ä¸“ä¸šé¢†åŸŸ: {', '.join(expert.expertise)}")
    
    # 2. æˆæœ¬ä¼°ç®—
    cost = expert.estimate_analysis_cost("PCSK9")
    print(f"\nğŸ’° åˆ†ææˆæœ¬ä¼°ç®—:")
    print(f"  é¢„ä¼°Token: {cost['estimated_tokens']}")
    print(f"  é¢„ä¼°æˆæœ¬: ${cost['estimated_cost_usd']:.4f}")
    print(f"  é¢„ä¼°æ—¶é—´: {cost['estimated_time_seconds']}ç§’")
    print(f"  æœ€å¤§æ–‡çŒ®æ•°: {cost['max_literature']}")
    
    # 3. æ‰§è¡Œåˆ†æ
    print(f"\nğŸ”¬ å¼€å§‹åˆ†æ PCSK9...")
    result = await expert.analyze("PCSK9")
    
    # 4. æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  åŸºå› : {result.gene_target}")
    print(f"  æ–‡çŒ®æ•°é‡: {result.total_literature}")
    print(f"  æ–‡æœ¬å—æ•°: {result.total_chunks}")
    print(f"  ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
    print(f"  åˆ†ææ–¹æ³•: {result.analysis_method}")
    
    print(f"\nğŸ¦  ç–¾ç—…æœºåˆ¶åˆ†æ:")
    print(result.disease_mechanism[:200] + "...")
    
    print(f"\nğŸ’Š æ²»ç–—ç­–ç•¥åˆ†æ:")
    print(result.treatment_strategy[:200] + "...")
    
    print(f"\nğŸ¯ é¶ç‚¹åˆ†æ:")
    print(result.target_analysis[:200] + "...")
    
    # 5. Tokenä½¿ç”¨ç»Ÿè®¡
    if result.token_usage:
        print(f"\nğŸ’¾ Tokenä½¿ç”¨ç»Ÿè®¡:")
        print(f"  æ€»è®¡: {result.token_usage['total_tokens']}")
        print(f"  è¾“å…¥: {result.token_usage['input_tokens']}")
        print(f"  è¾“å‡º: {result.token_usage['output_tokens']}")
    
    # 6. å¼•ç”¨æ–‡çŒ®
    print(f"\nğŸ“š å¼•ç”¨æ–‡çŒ® (å‰5ç¯‡):")
    for ref in result.references[:5]:
        print(f"  â€¢ {ref['Title']} (PMID: {ref['PMID']})")
    
    print(f"\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(example_usage())