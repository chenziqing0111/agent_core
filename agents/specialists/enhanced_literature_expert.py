#!/usr/bin/env python3
"""
ğŸ§¬ Enhanced Literature Expert - å¢å¼ºæ–‡çŒ®åˆ†æä¸“å®¶
æ”¯æŒåŸºå› åã€å…³é”®è¯ã€ä¸“ä¸šæœ¯è¯­ç­‰å¤šç§æŸ¥è¯¢æ–¹å¼
"""

import sys
import os
import asyncio
import hashlib
import pickle
import logging
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# æ ¸å¿ƒä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import faiss
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å®‰è£…: pip install sentence-transformers faiss-cpu")
    raise e

from Bio import Entrez
import xml.etree.ElementTree as ET

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from agent_core.clients.llm_client import call_llm
from agent_core.config.analysis_config import AnalysisConfig, AnalysisMode, ConfigManager

# å¯¼å…¥åŸºç¡€ç±»ï¼ˆé‡ç”¨ç°æœ‰çš„æ•°æ®ç»“æ„ï¼‰
from agent_core.agents.specialists.literature_expert import (
    LiteratureDocument, TextChunk, VectorStore, RAGProcessor, 
    CacheManager, SmartChunker, LiteratureAnalysisResult
)

logger = logging.getLogger(__name__)

# ===== æŸ¥è¯¢ç±»å‹æšä¸¾ =====

from enum import Enum

class QueryType(Enum):
    """æŸ¥è¯¢ç±»å‹"""
    GENE = "gene"                    # åŸºå› æŸ¥è¯¢
    KEYWORD = "keyword"              # å…³é”®è¯æŸ¥è¯¢
    PROTEIN_FAMILY = "protein_family" # è›‹ç™½å®¶æ—æŸ¥è¯¢
    MECHANISM = "mechanism"          # æœºåˆ¶æŸ¥è¯¢
    COMPLEX = "complex"              # å¤åˆæŸ¥è¯¢

@dataclass
class SearchQuery:
    """æœç´¢æŸ¥è¯¢ç»“æ„"""
    query_text: str                 # æŸ¥è¯¢æ–‡æœ¬
    query_type: QueryType           # æŸ¥è¯¢ç±»å‹
    additional_terms: List[str] = None  # é™„åŠ æœ¯è¯­
    exclude_terms: List[str] = None     # æ’é™¤æœ¯è¯­
    date_range: tuple = None            # æ—¥æœŸèŒƒå›´ (start_year, end_year)
    max_results: int = 500              # æœ€å¤§ç»“æœæ•°
    
    def __post_init__(self):
        if self.additional_terms is None:
            self.additional_terms = []
        if self.exclude_terms is None:
            self.exclude_terms = []

# ===== å¢å¼ºçš„PubMedæ£€ç´¢å™¨ =====

class EnhancedPubMedRetriever:
    """å¢å¼ºçš„PubMedæ–‡çŒ®æ£€ç´¢å™¨ - æ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹"""
    
    def __init__(self):
        self.name = "Enhanced PubMed Retriever"
        self.version = "3.0.0"
        # é…ç½®Bio.Entrez
        Entrez.email = "czqrainy@gmail.com"
        Entrez.api_key = "983222f9d5a2a81facd7d158791d933e6408"
        
        # é¢„å®šä¹‰çš„æœç´¢æ¨¡æ¿
        self.search_templates = {
            QueryType.GENE: [
                "{query}[Title/Abstract]",
                '"{query}" AND (disease OR treatment OR therapy)',
                "{query} AND (clinical trial[Publication Type] OR clinical study[Publication Type])",
                "{query} AND (mechanism OR pathway OR function)",
                "{query} AND (drug OR inhibitor OR target OR therapeutic)"
            ],
            QueryType.KEYWORD: [
                "{query}[Title/Abstract]",
                '"{query}" AND (regulation OR expression OR function)',
                "{query} AND (signaling OR pathway OR mechanism)",
                "{query} AND (therapeutic OR treatment OR clinical)",
                "{query} AND (protein OR gene OR molecular)"
            ],
            QueryType.PROTEIN_FAMILY: [
                '"{query}" AND (protein OR family OR domain)',
                "{query} AND (structure OR function OR binding)",
                "{query} AND (regulation OR expression OR localization)",
                "{query} AND (interaction OR complex OR assembly)",
                "{query} AND (evolution OR conservation OR phylogeny)"
            ],
            QueryType.MECHANISM: [
                '"{query}" AND (mechanism OR pathway OR process)',
                "{query} AND (regulation OR control OR modulation)",
                "{query} AND (signaling OR cascade OR network)",
                "{query} AND (molecular OR cellular OR biological)",
                "{query} AND (function OR role OR activity)"
            ],
            QueryType.COMPLEX: [
                "{query}",  # å¤åˆæŸ¥è¯¢ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                '"{query}" AND review[Publication Type]',
                "{query} AND recent[Filter]"
            ]
        }
    
    async def search_literature(self, search_query: Union[str, SearchQuery], max_results: int = 500) -> List[LiteratureDocument]:
        """
        æ£€ç´¢æ–‡çŒ® - æ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹
        
        Args:
            search_query: æŸ¥è¯¢å­—ç¬¦ä¸²æˆ–SearchQueryå¯¹è±¡
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            æ–‡çŒ®æ–‡æ¡£åˆ—è¡¨
        """
        
        # å¤„ç†è¾“å…¥å‚æ•°
        if isinstance(search_query, str):
            # å…¼å®¹åŸæœ‰æ¥å£ï¼šå­—ç¬¦ä¸²æŸ¥è¯¢é»˜è®¤ä¸ºåŸºå› æŸ¥è¯¢
            query = SearchQuery(
                query_text=search_query,
                query_type=QueryType.GENE,
                max_results=max_results
            )
        else:
            query = search_query
            max_results = query.max_results
        
        print(f"ğŸ“š æ£€ç´¢æ–‡çŒ®: {query.query_text} ({query.query_type.value})")
        print(f"   ç›®æ ‡: {max_results} ç¯‡")
        
        # æ„å»ºæœç´¢ç­–ç•¥
        search_strategies = self._build_search_strategies(query)
        
        all_documents = []
        seen_pmids = set()
        
        for i, strategy in enumerate(search_strategies):
            print(f"  ğŸ” æœç´¢ç­–ç•¥ {i+1}: {strategy}")
            
            try:
                docs = await self._execute_search(strategy, max_results // len(search_strategies))
                
                for doc in docs:
                    if doc.pmid not in seen_pmids:
                        seen_pmids.add(doc.pmid)
                        all_documents.append(doc)
                        
                        if len(all_documents) >= max_results:
                            break
                
                print(f"    âœ… æ–°å¢ {len(docs)} ç¯‡ï¼Œç´¯è®¡ {len(all_documents)} ç¯‡")
                
                if len(all_documents) >= max_results:
                    break
                    
            except Exception as e:
                print(f"    âŒ æœç´¢å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“Š æ£€ç´¢å®Œæˆ: å…± {len(all_documents)} ç¯‡æ–‡çŒ®")
        return all_documents[:max_results]
    
    def _build_search_strategies(self, query: SearchQuery) -> List[str]:
        """æ„å»ºæœç´¢ç­–ç•¥"""
        
        base_strategies = self.search_templates.get(query.query_type, self.search_templates[QueryType.KEYWORD])
        strategies = []
        
        # åŸºç¡€æŸ¥è¯¢ç­–ç•¥
        for template in base_strategies:
            strategy = template.format(query=query.query_text)
            strategies.append(strategy)
        
        # æ·»åŠ é™„åŠ æœ¯è¯­
        if query.additional_terms:
            additional_query = f"({query.query_text}) AND ({' OR '.join(query.additional_terms)})"
            strategies.append(additional_query)
        
        # å¤„ç†æ’é™¤æœ¯è¯­
        if query.exclude_terms:
            exclude_part = " AND ".join([f"NOT {term}" for term in query.exclude_terms])
            enhanced_strategies = []
            for strategy in strategies[:2]:  # åªå¯¹å‰ä¸¤ä¸ªç­–ç•¥åº”ç”¨æ’é™¤
                enhanced_strategies.append(f"{strategy} {exclude_part}")
            strategies.extend(enhanced_strategies)
        
        # æ—¥æœŸèŒƒå›´è¿‡æ»¤
        if query.date_range:
            start_year, end_year = query.date_range
            date_filter = f" AND {start_year}[PDAT]:{end_year}[PDAT]"
            dated_strategies = []
            for strategy in strategies[:3]:  # å¯¹å‰ä¸‰ä¸ªç­–ç•¥åº”ç”¨æ—¥æœŸè¿‡æ»¤
                dated_strategies.append(f"{strategy}{date_filter}")
            strategies.extend(dated_strategies)
        
        return strategies
    
    async def _execute_search(self, query: str, max_results: int) -> List[LiteratureDocument]:
        """æ‰§è¡Œå•æ¬¡æœç´¢"""
        
        try:
            # 1. æœç´¢PMID
            search_handle = Entrez.esearch(
                db="pubmed", 
                term=query, 
                retmax=max_results,
                sort="relevance"
            )
            search_results = Entrez.read(search_handle)
            pmid_list = search_results["IdList"]
            
            if not pmid_list:
                return []
            
            # 2. æ‰¹é‡è·å–è¯¦æƒ…
            documents = []
            batch_size = 50
            
            for i in range(0, len(pmid_list), batch_size):
                batch_pmids = pmid_list[i:i+batch_size]
                batch_docs = await self._fetch_batch_details(batch_pmids)
                documents.extend(batch_docs)
                
                # APIé™æµ
                if i + batch_size < len(pmid_list):
                    await asyncio.sleep(0.5)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æœç´¢æ‰§è¡Œå¤±è´¥: {e}")
            return []
    
    async def _fetch_batch_details(self, pmid_list: List[str]) -> List[LiteratureDocument]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…"""
        
        try:
            fetch_handle = Entrez.efetch(
                db="pubmed",
                id=",".join(pmid_list),
                rettype="medline",
                retmode="xml"
            )
            
            xml_data = fetch_handle.read()
            fetch_handle.close()
            
            # è§£æXML
            root = ET.fromstring(xml_data)
            articles = root.findall(".//PubmedArticle")
            
            documents = []
            for article_xml in articles:
                doc = self._parse_article(article_xml)
                if doc:
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_article(self, article_xml) -> Optional[LiteratureDocument]:
        """è§£æå•ç¯‡æ–‡ç« """
        
        try:
            # åŸºæœ¬ä¿¡æ¯
            pmid = article_xml.findtext(".//PMID", "")
            title = article_xml.findtext(".//ArticleTitle", "")
            
            # æ‘˜è¦å¤„ç†
            abstract_elem = article_xml.find(".//Abstract")
            abstract = ""
            if abstract_elem is not None:
                abstract_texts = []
                for text_elem in abstract_elem.findall(".//AbstractText"):
                    text = text_elem.text or ""
                    label = text_elem.get("Label", "")
                    if label:
                        abstract_texts.append(f"{label}: {text}")
                    else:
                        abstract_texts.append(text)
                abstract = " ".join(abstract_texts)
            
            # ä½œè€…
            authors = []
            for author in article_xml.findall(".//Author"):
                last_name = author.findtext("LastName", "")
                first_name = author.findtext("ForeName", "")
                if last_name:
                    authors.append(f"{first_name} {last_name}".strip())
            
            # æœŸåˆŠå’Œå¹´ä»½
            journal = article_xml.findtext(".//Journal/Title", "")
            year_elem = article_xml.find(".//PubDate/Year")
            year = int(year_elem.text) if year_elem is not None and year_elem.text else 0
            
            # DOI
            doi = ""
            for article_id in article_xml.findall(".//ArticleId"):
                if article_id.get("IdType") == "doi":
                    doi = article_id.text or ""
                    break
            
            if not title or not abstract:
                return None
            
            return LiteratureDocument(
                pmid=pmid,
                title=title,
                abstract=abstract,
                authors=authors,
                journal=journal,
                year=year,
                doi=doi
            )
            
        except Exception as e:
            return None

# ===== å¢å¼ºçš„æ–‡çŒ®åˆ†æä¸“å®¶ =====

class EnhancedLiteratureExpert:
    """å¢å¼ºæ–‡çŒ®åˆ†æä¸“å®¶ - æ”¯æŒå¤šç§æŸ¥è¯¢æ–¹å¼"""
    
    def __init__(self, config: AnalysisConfig = None):
        self.name = "Enhanced Literature Expert"
        self.version = "3.0.0"
        self.expertise = ["å¤šç±»å‹æŸ¥è¯¢", "æ–‡çŒ®åˆ†æ", "æœºåˆ¶ç ”ç©¶", "æ²»ç–—ç­–ç•¥", "é¶ç‚¹åˆ†æ"]
        
        # é…ç½®
        self.config = config or ConfigManager.get_standard_config()
        
        # ç»„ä»¶
        self.retriever = EnhancedPubMedRetriever()
        self.chunker = SmartChunker(chunk_size=250, overlap=50)
        self.cache_manager = EnhancedCacheManager()
        
        logger.info(f"Enhanced Literature Expert åˆå§‹åŒ–å®Œæˆ - {self.version}")
    
    def set_config(self, config: AnalysisConfig):
        """è®¾ç½®é…ç½®"""
        self.config = config
        logger.info(f"é…ç½®å·²æ›´æ–°: {config.mode.value}")
    
    def set_mode(self, mode: AnalysisMode):
        """è®¾ç½®æ¨¡å¼"""
        self.config = ConfigManager.get_config_by_mode(mode)
        logger.info(f"æ¨¡å¼åˆ‡æ¢: {mode.value}")
    
    async def analyze_by_gene(self, gene_target: str, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """åŸºå› ååˆ†æï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        
        query = SearchQuery(
            query_text=gene_target,
            query_type=QueryType.GENE,
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_by_keyword(self, keyword: str, 
                                additional_terms: List[str] = None,
                                exclude_terms: List[str] = None,
                                context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """å…³é”®è¯åˆ†æ"""
        
        query = SearchQuery(
            query_text=keyword,
            query_type=QueryType.KEYWORD,
            additional_terms=additional_terms or [],
            exclude_terms=exclude_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_protein_family(self, family_name: str,
                                   additional_terms: List[str] = None,
                                   context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """è›‹ç™½å®¶æ—åˆ†æ"""
        
        query = SearchQuery(
            query_text=family_name,
            query_type=QueryType.PROTEIN_FAMILY,
            additional_terms=additional_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_mechanism(self, mechanism_query: str,
                              additional_terms: List[str] = None,
                              context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """æœºåˆ¶åˆ†æ"""
        
        query = SearchQuery(
            query_text=mechanism_query,
            query_type=QueryType.MECHANISM,
            additional_terms=additional_terms or [],
            max_results=self._get_max_literature()
        )
        
        return await self.analyze_by_query(query, context)
    
    async def analyze_by_query(self, search_query: SearchQuery, context: Dict[str, Any] = None) -> LiteratureAnalysisResult:
        """
        é€šç”¨æŸ¥è¯¢åˆ†ææ–¹æ³•
        
        Args:
            search_query: æœç´¢æŸ¥è¯¢å¯¹è±¡
            context: ä¸Šä¸‹æ–‡é…ç½®
        
        Returns:
            æ–‡çŒ®åˆ†æç»“æœ
        """
        
        logger.info(f"å¼€å§‹æ–‡çŒ®åˆ†æ: {search_query.query_text} ({search_query.query_type.value}) - æ¨¡å¼: {self.config.mode.value}")
        
        try:
            # ç¡®å®šåˆ†æå‚æ•°
            top_k = self._get_top_k()
            
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
            cache_key = self._generate_cache_key(search_query)
            vector_store = self.cache_manager.load_by_key(cache_key)
            
            # 2. å¦‚æœç¼“å­˜æ— æ•ˆï¼Œé‡æ–°æ„å»º
            if vector_store is None:
                vector_store = await self._build_literature_index(search_query)
                # ä¿å­˜ç¼“å­˜
                self.cache_manager.save_by_key(cache_key, vector_store)
            
            # 3. RAGæŸ¥è¯¢å¤„ç†
            rag_processor = RAGProcessor(vector_store)
            
            print("ğŸ¤– å¼€å§‹RAGæŸ¥è¯¢...")
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´RAGæŸ¥è¯¢
            rag_queries = self._get_rag_queries(search_query)
            
            # å¹¶å‘å¤„ç†æŸ¥è¯¢
            tasks = [
                rag_processor.process_query(search_query.query_text, query_type, top_k)
                for query_type in rag_queries
            ]
            
            results = await asyncio.gather(*tasks)
            
            # 4. æ„å»ºåˆ†æç»“æœ
            references = self._extract_references(vector_store.chunks)
            confidence_score = self._calculate_confidence(vector_store.chunks)
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹ç»„ç»‡ç»“æœ
            result_dict = {}
            for i, query_type in enumerate(rag_queries):
                result_dict[query_type] = results[i] if i < len(results) else ""
            
            analysis_result = LiteratureAnalysisResult(
                gene_target=search_query.query_text,  # ä¿æŒå…¼å®¹æ€§
                disease_mechanism=result_dict.get("disease_mechanism", ""),
                treatment_strategy=result_dict.get("treatment_strategy", ""),
                target_analysis=result_dict.get("target_analysis", ""),
                references=references[:50],  # é™åˆ¶å¼•ç”¨æ•°é‡
                total_literature=len(set(chunk.doc_id for chunk in vector_store.chunks)),
                total_chunks=len(vector_store.chunks),
                confidence_score=confidence_score,
                analysis_method=f"Enhanced-RAG-{search_query.query_type.value}",
                timestamp=datetime.now().isoformat(),
                config_used=self._get_config_summary(),
                token_usage=self._estimate_token_usage(top_k)
            )
            
            logger.info(f"æ–‡çŒ®åˆ†æå®Œæˆ: {search_query.query_text} - æ–‡çŒ®æ•°: {analysis_result.total_literature}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"æ–‡çŒ®åˆ†æå¤±è´¥: {search_query.query_text} - {str(e)}")
            return self._create_error_result(search_query.query_text, str(e))
    
    def _get_rag_queries(self, search_query: SearchQuery) -> List[str]:
        """æ ¹æ®æŸ¥è¯¢ç±»å‹è·å–RAGæŸ¥è¯¢ç±»å‹"""
        
        if search_query.query_type == QueryType.GENE:
            return ["disease_mechanism", "treatment_strategy", "target_analysis"]
        elif search_query.query_type == QueryType.KEYWORD:
            # ä½¿ç”¨å…¼å®¹çš„æŸ¥è¯¢ç±»å‹
            return ["disease_mechanism", "treatment_strategy", "target_analysis"]
        elif search_query.query_type == QueryType.PROTEIN_FAMILY:
            # ä½¿ç”¨å…¼å®¹çš„æŸ¥è¯¢ç±»å‹
            return ["disease_mechanism", "treatment_strategy", "target_analysis"]
        elif search_query.query_type == QueryType.MECHANISM:
            # ä½¿ç”¨å…¼å®¹çš„æŸ¥è¯¢ç±»å‹
            return ["disease_mechanism", "treatment_strategy", "target_analysis"]
        else:
            return ["disease_mechanism", "treatment_strategy", "target_analysis"]
    
    async def _build_literature_index(self, search_query: SearchQuery) -> VectorStore:
        """æ„å»ºæ–‡çŒ®ç´¢å¼•"""
        
        print(f"ğŸ—ï¸ æ„å»ºæ–‡çŒ®ç´¢å¼•: {search_query.query_text} ({search_query.query_type.value})")
        
        # 1. æ£€ç´¢æ–‡çŒ®
        documents = await self.retriever.search_literature(search_query)
        
        if not documents:
            raise ValueError(f"æœªæ‰¾åˆ° {search_query.query_text} ç›¸å…³æ–‡çŒ®")
        
        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.chunker.chunk_documents(documents)
        
        # 3. æ„å»ºå‘é‡ç´¢å¼•
        vector_store = VectorStore()
        vector_store.build_index(chunks)
        
        return vector_store
    
    def _generate_cache_key(self, search_query: SearchQuery) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        
        query_str = f"{search_query.query_text}_{search_query.query_type.value}"
        if search_query.additional_terms:
            query_str += f"_add_{','.join(search_query.additional_terms)}"
        if search_query.exclude_terms:
            query_str += f"_exc_{','.join(search_query.exclude_terms)}"
        if search_query.date_range:
            query_str += f"_date_{search_query.date_range[0]}_{search_query.date_range[1]}"
        
        return hashlib.md5(query_str.encode()).hexdigest()
    
    def _get_max_literature(self) -> int:
        """è·å–æœ€å¤§æ–‡çŒ®æ•°é‡"""
        if self.config.mode == AnalysisMode.QUICK:
            return 100
        elif self.config.mode == AnalysisMode.STANDARD:
            return 500
        elif self.config.mode == AnalysisMode.DEEP:
            return 1000
        else:
            return 500
    
    def _get_top_k(self) -> int:
        """è·å–top-kå‚æ•°"""
        if self.config.mode == AnalysisMode.QUICK:
            return 10
        elif self.config.mode == AnalysisMode.STANDARD:
            return 15
        elif self.config.mode == AnalysisMode.DEEP:
            return 25
        else:
            return 15
    
    def _extract_references(self, chunks: List[TextChunk]) -> List[Dict]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        
        references = {}
        for chunk in chunks:
            pmid = chunk.metadata.get("pmid", "")
            if pmid and pmid not in references:
                references[pmid] = {
                    "PMID": pmid,
                    "Title": chunk.metadata.get("title", ""),
                    "Journal": chunk.metadata.get("journal", ""),
                    "Year": chunk.metadata.get("year", 0)
                }
        
        return list(references.values())
    
    def _calculate_confidence(self, chunks: List[TextChunk]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        
        if not chunks:
            return 0.0
        
        # åŸºäºæ–‡çŒ®æ•°é‡ã€è´¨é‡ç­‰å› ç´ è®¡ç®—ç½®ä¿¡åº¦
        unique_docs = len(set(chunk.doc_id for chunk in chunks))
        recent_docs = sum(1 for chunk in chunks if chunk.metadata.get("year", 0) >= 2020)
        
        base_confidence = min(unique_docs / 100, 1.0)  # åŸºç¡€ç½®ä¿¡åº¦
        recency_bonus = (recent_docs / unique_docs) * 0.2 if unique_docs > 0 else 0  # æ—¶æ•ˆæ€§åŠ åˆ†
        
        return min(base_confidence + recency_bonus, 1.0)
    
    def _get_config_summary(self) -> Dict:
        """è·å–é…ç½®æ‘˜è¦"""
        
        return {
            "mode": self.config.mode.value,
            "max_literature": self._get_max_literature(),
            "top_k": self._get_top_k()
        }
    
    def _estimate_token_usage(self, top_k: int) -> Dict:
        """ä¼°ç®—Tokenä½¿ç”¨é‡"""
        
        return {
            "estimated_input_tokens": top_k * 200,  # æ¯ä¸ªchunkçº¦200 tokens
            "estimated_output_tokens": 1500,       # è¾“å‡ºçº¦1500 tokens
            "total_estimated": top_k * 200 + 1500
        }
    
    def _create_error_result(self, query_text: str, error_msg: str) -> LiteratureAnalysisResult:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        
        return LiteratureAnalysisResult(
            gene_target=query_text,
            disease_mechanism=f"åˆ†æå¤±è´¥: {error_msg}",
            treatment_strategy="",
            target_analysis="",
            references=[],
            total_literature=0,
            total_chunks=0,
            confidence_score=0.0,
            analysis_method="error",
            timestamp=datetime.now().isoformat(),
            config_used={},
            token_usage={}
        )

# ===== ç¼“å­˜ç®¡ç†å™¨æ‰©å±• =====

class EnhancedCacheManager:
    """å¢å¼ºçš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "enhanced_literature_cache"):
        self.cache_dir = cache_dir
        self.cache_days = 7  # ç¼“å­˜æœ‰æ•ˆæœŸ
        os.makedirs(cache_dir, exist_ok=True)
    
    def load_by_key(self, cache_key: str) -> Optional[VectorStore]:
        """æ ¹æ®ç¼“å­˜é”®åŠ è½½"""
        
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if not os.path.exists(cache_file):
            return None
        
        try:
            # æ£€æŸ¥ç¼“å­˜æ—¶æ•ˆ
            mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file))
            if datetime.now() - mod_time > timedelta(days=self.cache_days):
                return None
            
            with open(cache_file, 'rb') as f:
                vector_store = pickle.load(f)
                print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½: {cache_key}")
                return vector_store
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_by_key(self, cache_key: str, vector_store: VectorStore):
        """æ ¹æ®ç¼“å­˜é”®ä¿å­˜"""
        
        try:
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump(vector_store, f)
            print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_key}")
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # å…¼å®¹åŸæœ‰æ¥å£
    def load(self, gene: str, max_results: int) -> Optional[VectorStore]:
        """å…¼å®¹åŸæœ‰loadæ–¹æ³•"""
        cache_key = f"{gene}_{max_results}"
        return self.load_by_key(cache_key)
    
    def save(self, gene: str, max_results: int, vector_store: VectorStore):
        """å…¼å®¹åŸæœ‰saveæ–¹æ³•"""
        cache_key = f"{gene}_{max_results}"
        self.save_by_key(cache_key, vector_store)

# ===== ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•° =====

async def test_enhanced_literature_expert():
    """æµ‹è¯•å¢å¼ºæ–‡çŒ®ä¸“å®¶"""
    
    expert = EnhancedLiteratureExpert()
    
    print("ğŸ§ª æµ‹è¯•1: KRAB-like è›‹ç™½æŸ¥è¯¢")
    result1 = await expert.analyze_by_keyword(
        keyword="krab-like",
        additional_terms=["epigenetic", "transcriptional regulation", "zinc finger"],
        exclude_terms=["virus", "bacterial"]
    )
    
    print(f"ç»“æœ: {result1.total_literature} ç¯‡æ–‡çŒ®")
    
    print("\nğŸ§ª æµ‹è¯•2: è›‹ç™½å®¶æ—æŸ¥è¯¢")
    result2 = await expert.analyze_protein_family(
        family_name="KRAB domain proteins",
        additional_terms=["chromatin modification", "gene silencing"]
    )
    
    print(f"ç»“æœ: {result2.total_literature} ç¯‡æ–‡çŒ®")
    
    print("\nğŸ§ª æµ‹è¯•3: æœºåˆ¶æŸ¥è¯¢")
    result3 = await expert.analyze_mechanism(
        mechanism_query="epigenetic regulation by zinc finger proteins",
        additional_terms=["DNA methylation", "histone modification"]
    )
    
    print(f"ç»“æœ: {result3.total_literature} ç¯‡æ–‡çŒ®")
    
    return [result1, result2, result3]

if __name__ == "__main__":
    asyncio.run(test_enhanced_literature_expert())